import os
from dotenv import load_dotenv
from supabase import create_client, Client
from sentence_transformers import SentenceTransformer

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# 2. è¿æ¥æ•°æ®åº“
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# 3. åŠ è½½æ¨¡å‹ (è¿™ä¸ªå¾ˆå¿«ï¼Œå› ä¸ºæ¨¡å‹åˆšæ‰å·²ç»ä¸‹è½½è¿‡äº†)
print("â³ æ­£åœ¨åŠ è½½ AI æ¨¡å‹...")
model = SentenceTransformer('shibing624/text2vec-base-chinese')

def search_law(query_text: str):
    print(f"\nğŸ” æ­£åœ¨æœç´¢: {query_text}")
    
    # 1. æŠŠé—®é¢˜å˜æˆå‘é‡
    query_vector = model.encode(query_text).tolist()
    
    # 2. å» Supabase æœç´¢æœ€ç›¸ä¼¼çš„æ¡æ¬¾
    # rpc æ˜¯ "Remote Procedure Call" çš„ç¼©å†™ï¼Œå°±æ˜¯è°ƒç”¨æˆ‘ä»¬åœ¨ SQL é‡Œå†™çš„å‡½æ•°
    response = supabase.rpc("match_documents", {
        "query_embedding": query_vector,
        "match_threshold": 0.5, # ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)ï¼Œè¶Šä½æœåˆ°çš„è¶Šå¤šä½†è¶Šä¸å‡†
        "match_count": 3        # åªè¿”å›å‰ 3 æ¡
    }).execute()
    
    # 3. æ‰“å°ç»“æœ
    if response.data:
        for i, doc in enumerate(response.data):
            print(f"\n--- ç»“æœ {i+1} (ç›¸ä¼¼åº¦: {doc['similarity']:.4f}) ---")
            print(f"ã€å‡ºå¤„ã€‘{doc['law_name']} - {doc['reference_id']}")
            print(f"ã€å†…å®¹ã€‘{doc['content']}")
    else:
        print("ğŸ¤·â€â™‚ï¸ æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡ã€‚")

if __name__ == "__main__":
    # åœ¨è¿™é‡Œä¿®æ”¹ä½ æƒ³é—®çš„é—®é¢˜
    questions = [
        "é«˜ç©ºæŠ›ç‰©æ€ä¹ˆå®šè´£ï¼Ÿ",
        "ç¦»å©šæ—¶è´¢äº§æ€ä¹ˆåˆ†å‰²ï¼Ÿ",
        "ç§Ÿæˆ¿åˆåŒè¿˜æ²¡åˆ°æœŸæˆ¿ä¸œè¦èµ¶æˆ‘èµ°æ€ä¹ˆåŠï¼Ÿ"
    ]
    
    for q in questions:
        search_law(q)