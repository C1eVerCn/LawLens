import os
import re
import time
from typing import List, Dict
from dotenv import load_dotenv
from supabase import create_client, Client
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ é”™è¯¯: è¯·å…ˆåœ¨ .env æ–‡ä»¶é‡Œå¡«å¥½ Supabase çš„ç½‘å€å’Œå¯†é’¥ï¼")
    exit()

# 2. è¿æ¥æ•°æ®åº“
try:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
except Exception as e:
    print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
    exit()

print("â³ æ­£åœ¨ä¸‹è½½/åŠ è½½ AI æ¨¡å‹ (ç¬¬ä¸€æ¬¡è¿è¡Œä¼šæ¯”è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
# è¿™é‡Œä¼šä¸‹è½½ä¸€ä¸ªå‡ ç™¾MBçš„å…è´¹æ¨¡å‹åˆ°ä½ æœ¬åœ°
model = SentenceTransformer('shibing624/text2vec-base-chinese') 

def parse_law_text(file_path: str, law_name: str) -> List[Dict]:
    """è¯»å– txt æ–‡ä»¶å¹¶åˆ‡åˆ†æˆæ¡æ¬¾"""
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {file_path}")
        return []

    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()

    # åˆ‡åˆ†è§„åˆ™ï¼šæŒ‰ "ç¬¬Xæ¡" åˆ‡åˆ†
    pattern = r"(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+æ¡\s+)"
    parts = re.split(pattern, text)
    
    docs = []
    current_clause = ""
    
    print(f"ğŸ“„ æ­£åœ¨è§£æ: {law_name}...")
    for part in parts:
        if re.match(pattern, part):
            current_clause = part.strip()
        else:
            if current_clause and part.strip():
                # æŠŠ "ç¬¬ä¸€æ¡" å’Œ "å†…å®¹" æ‹¼èµ·æ¥
                docs.append({
                    "content": f"{current_clause} {part.strip()}",
                    "law_name": law_name,
                    "reference_id": current_clause,
                    "category": "law"
                })
                current_clause = ""
    
    return docs

def ingest_data(docs: List[Dict]):
    """æŠŠæ•°æ®ä¸Šä¼ åˆ° Supabase"""
    if not docs: return
    print(f"ğŸš€ å‡†å¤‡ä¸Šä¼  {len(docs)} æ¡æ•°æ®...")
    
    batch_size = 10 # æ¯æ¬¡ä¼ 10æ¡ï¼Œç¨³ä¸€ç‚¹
    for i in tqdm(range(0, len(docs), batch_size)):
        batch = docs[i : i + batch_size]
        
        # 1. AI å°†æ–‡æœ¬è½¬åŒ–ä¸ºå‘é‡
        texts = [d["content"] for d in batch]
        embeddings = model.encode(texts)
        
        # 2. å‡†å¤‡è¦å­˜çš„æ•°æ®
        records = []
        for doc, emb in zip(batch, embeddings):
            records.append({
                "content": doc["content"],
                "law_name": doc["law_name"],
                "reference_id": doc["reference_id"],
                "category": "law"
                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å…ˆæŠŠ embedding è½¬æˆ list
                , "embedding": emb.tolist() 
            })
            
        # 3. å‘é€åˆ° Supabase
        try:
            supabase.table("legal_docs").insert(records).execute()
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¼ å‡ºé”™: {e}")
            # å¦‚æœå‡ºé”™ç¨å¾®ç­‰ä¸€ä¸‹å†è¯•
            time.sleep(1)

if __name__ == "__main__":
    # è¿™é‡ŒæŒ‡å®šä½ è¦å¤„ç†çš„æ–‡ä»¶
    # è¯·ç¡®ä¿æŠŠ txt æ–‡ä»¶æ”¾åœ¨ data æ–‡ä»¶å¤¹ä¸‹
    txt_file = "data/minfadian.txt"
    
    if os.path.exists(txt_file):
        documents = parse_law_text(txt_file, "ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸")
        ingest_data(documents)
        print("ğŸ‰ æ­å–œï¼æ•°æ®å…¥åº“å®Œæˆï¼")
    else:
        print(f"âš ï¸ è¯·å…ˆä¸‹è½½æ°‘æ³•å…¸æ–‡æœ¬ï¼Œå¹¶é‡å‘½åä¸º minfadian.txt æ”¾å…¥ data æ–‡ä»¶å¤¹ï¼")