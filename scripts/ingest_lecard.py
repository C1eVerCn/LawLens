import os
import json
import time
from typing import List, Dict
from dotenv import load_dotenv
from supabase import create_client, Client
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# 1. åŠ è½½é…ç½®
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ é”™è¯¯: è¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
    exit()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
model = SentenceTransformer('shibing624/text2vec-base-chinese')

def chunk_text(text: str, chunk_size=400, overlap=50) -> List[str]:
    """ é•¿æ–‡æœ¬åˆ‡ç‰‡ï¼šé¿å…è¶…è¿‡æ¨¡å‹å¤„ç†é•¿åº¦ """
    if not text: return []
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        segment = text[start:end]
        chunks.append(segment)
        start += (chunk_size - overlap)
    return chunks

def process_lecard_json(file_path: str) -> List[Dict]:
    """ è§£æ LeCaRD æ ¼å¼çš„ JSON æ–‡ä»¶ """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # LeCaRDv1/v2 çš„å­—æ®µå¯èƒ½ç•¥æœ‰ä¸åŒï¼Œè¿™é‡Œåšå…¼å®¹å¤„ç†
        # ä¼˜å…ˆè·å– 'qw'(å…¨æ–‡)ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•æ‹¼æ¥ 'ajjbqk'(æ¡ˆæƒ…) + 'pjjg'(åˆ¤å†³)
        content = data.get('qw', '')
        if not content:
            content = (data.get('ajjbqk', '') + "\n" + data.get('pjjg', '')).strip()
            
        case_name = data.get('ajName', os.path.basename(file_path))
        
        if not content:
            return []

        # å¯¹é•¿æ¡ˆæƒ…è¿›è¡Œåˆ‡ç‰‡
        chunks = chunk_text(content)
        records = []
        
        for chunk in chunks:
            records.append({
                "content": chunk,
                "law_name": case_name,   # å­˜å…¥æ¡ˆä»¶åç§°
                "reference_id": "çœŸå®æ¡ˆä¾‹", # æ ‡è®°æ¥æº
                "category": "case",      # å…³é”®åˆ†ç±»ï¼šcase
                "meta": {"source": "LeCaRD"} # é¢å¤–å…ƒæ•°æ®(å¯é€‰)
            })
            
        return records
        
    except Exception as e:
        print(f"âš ï¸ è§£æé”™è¯¯ {file_path}: {e}")
        return []

def ingest_folder(folder_path: str):
    """ éå†æ–‡ä»¶å¤¹å¹¶å…¥åº“ """
    print(f"ğŸ“‚ æ­£åœ¨æ‰«ææ–‡ä»¶å¤¹: {folder_path} ...")
    
    files = []
    for root, _, filenames in os.walk(folder_path):
        for filename in filenames:
            if filename.endswith('.json'):
                files.append(os.path.join(root, filename))
    
    print(f"ğŸ“Š å‘ç° {len(files)} ä¸ªæ¡ˆä¾‹æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†...")
    
    # æ‰¹é‡å¤„ç†
    batch_records = []
    total_inserted = 0
    
    for file_path in tqdm(files):
        records = process_lecard_json(file_path)
        
        # 1. å‘é‡åŒ– (Embedding)
        if records:
            texts = [r["content"] for r in records]
            embeddings = model.encode(texts)
            
            # æŠŠå‘é‡å¡å›è®°å½•é‡Œ
            for record, emb in zip(records, embeddings):
                record["embedding"] = emb.tolist()
                batch_records.append(record)
        
        # 2. æ¯ç§¯æ”’ 50 æ¡æ•°æ®å°±ä¸Šä¼ ä¸€æ¬¡ (é¿å…è¯·æ±‚å¤ªé¢‘ç¹)
        if len(batch_records) >= 50:
            try:
                supabase.table("legal_docs").insert(batch_records).execute()
                total_inserted += len(batch_records)
                batch_records = [] # æ¸…ç©ºç¼“å†²åŒº
            except Exception as e:
                print(f"âš ï¸ ä¸Šä¼ å¤±è´¥: {e}")
                time.sleep(1)

    # å¤„ç†å‰©ä½™çš„æ•°æ®
    if batch_records:
        supabase.table("legal_docs").insert(batch_records).execute()
        total_inserted += len(batch_records)

    print(f"ğŸ‰ å…¥åº“å®Œæˆï¼å…±ä¸Šä¼  {total_inserted} æ¡æ¡ˆä¾‹ç‰‡æ®µã€‚")

if __name__ == "__main__":
    # æŒ‡å®šä½ çš„æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
    data_dir = "data/lecard_cases"
    
    if os.path.exists(data_dir):
        ingest_folder(data_dir)
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶å¤¹: {data_dir}")
        print("è¯·å…ˆä¸‹è½½ LeCaRD æ•°æ®å¹¶æ”¾å…¥è¯¥æ–‡ä»¶å¤¹ï¼")