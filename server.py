import os
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse 
from pydantic import BaseModel
from supabase import create_client, Client
from openai import OpenAI
from typing import List, Optional
import json
import time

# ===========================
# 1. é…ç½®ä¸åˆå§‹åŒ– (å®Œå…¨ä¿æŒåŸæ ·)
# ===========================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")

supabase: Optional[Client] = None
client: Optional[OpenAI] = None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
def startup_event():
    global supabase, client
    if not all([SUPABASE_URL, SUPABASE_KEY, SILICONFLOW_API_KEY]):
        print("âŒ é”™è¯¯ï¼šæ ¸å¿ƒç¯å¢ƒå˜é‡ç¼ºå¤±")
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="https://api.siliconflow.cn/v1"
        )
        print("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (SiliconFlow / Qwen)")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

# ===========================
# 2. æ•°æ®æ¨¡å‹ (Pydantic) (å®Œå…¨ä¿æŒåŸæ ·)
# ===========================
class ChatMessage(BaseModel):
    role: str
    content: str

class AnalyzeRequest(BaseModel):
    messages: List[ChatMessage]
    current_doc: str = ""
    selection: Optional[str] = "" 
    mode: str = "draft"           # draft(ç”Ÿæˆ) | polish(æ¶¦è‰²) | selection_polish(å±€éƒ¨)

class DocumentSave(BaseModel):
    title: str
    content: str
    user_id: Optional[str] = None

# ===========================
# 3. è¾…åŠ©æ¥å£ (å†å² & ä¿å­˜) - (å®Œå…¨ä¿æŒåŸæ ·)
# ===========================
@app.post("/api/save")
async def save_document(doc: DocumentSave):
    """ä¿å­˜æ–‡æ¡£åˆ° Supabase"""
    if not supabase: return {"status": "error", "msg": "DBæœªè¿æ¥"}
    try:
        # æ™ºèƒ½æˆªå–æ ‡é¢˜
        title = doc.title
        if not title or title == "æœªå‘½åæ–‡æ¡£":
             # ç®€å•è¿‡æ»¤ HTML æ ‡ç­¾å–å‰ 15 å­—
             clean_text = doc.content.replace('<', '').replace('>', '')[:15]
             title = f"{clean_text}..."
             
        data = {"title": title, "content": doc.content, "user_id": doc.user_id}
        supabase.table("documents").insert(data).execute()
        return {"status": "success"}
    except Exception as e:
        print(f"Save error: {e}")
        return {"status": "error", "msg": str(e)}

@app.get("/api/history")
async def get_history(user_id: Optional[str] = None):
    """è·å–å†å²è®°å½•"""
    if not supabase: return []
    try:
        query = supabase.table("documents").select("*").order("created_at", desc=True).limit(20)
        if user_id: query = query.eq("user_id", user_id)
        else: query = query.is_("user_id", "null")
        res = query.execute()
        return res.data
    except Exception as e:
        print(f"History error: {e}")
        return []

# ===========================
# 4. æ ¸å¿ƒ AI ä¸šåŠ¡é€»è¾‘ (å‡çº§ç‰ˆ)
# ===========================

def get_relevant_laws_formatted(query: str):
    """
    RAG æ£€ç´¢é€»è¾‘ - å‡çº§ç‰ˆ
    ä¸ä»…æ£€ç´¢ï¼Œè¿˜è´Ÿè´£å°†ç»“æœæ ¼å¼åŒ–ä¸ºå¸¦ç¼–å·çš„å¼•ç”¨å—
    """
    if not client or not supabase: return ""
    try:
        # 1. å‘é‡åŒ–
        response = client.embeddings.create(model="BAAI/bge-m3", input=query)
        query_vector = response.data[0].embedding
        
        # 2. æ•°æ®åº“æ£€ç´¢ (æé«˜ä¸€ç‚¹é˜ˆå€¼ï¼Œä¿è¯è´¨é‡)
        rpc_response = supabase.rpc("match_documents", {
            "query_embedding": query_vector,
            "match_threshold": 0.45, 
            "match_count": 4 
        }).execute()
        
        data = rpc_response.data
        if not data: return ""

        # 3. æ ¼å¼åŒ–ä¸ºå¼•ç”¨æºå­—ç¬¦ä¸²
        formatted_sources = []
        for idx, doc in enumerate(data):
            # å°è¯•è·å–æ¥æºå­—æ®µï¼Œå…¼å®¹ä¸åŒè¡¨ç»“æ„
            meta = doc.get('metadata', {}) or {}
            source_name = doc.get('law_name') or meta.get('source') or "æ³•å¾‹æ•°æ®åº“"
            content_snippet = doc['content'][:500].replace("\n", " ") # å‹ç¼©ä¸€ä¸‹é˜²æ­¢ token çˆ†ç‚¸
            
            block = f"[å‚è€ƒèµ„æ–™ {idx + 1}] æ¥æºï¼š{source_name}\nå†…å®¹ï¼š{content_snippet}..."
            formatted_sources.append(block)
            
        return "\n\n".join(formatted_sources)

    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        return ""

@app.post("/api/analyze")
async def analyze(request: AnalyzeRequest):
    """æ ¸å¿ƒ AI åˆ†ææ¥å£ (æµå¼å“åº”)"""
    last_user_msg = request.selection or request.messages[-1].content
    print(f"ğŸ” [AI] æ¨¡å¼: {request.mode} | æ„å›¾: {last_user_msg[:20]}...")
    
    # --- 1. RAG æ£€ç´¢ä¸ä¸Šä¸‹æ–‡æ„å»º ---
    context_text = ""
    # å±€éƒ¨æ¶¦è‰²ä¸€èˆ¬ä¸éœ€è¦æŸ¥å¤§æ¡ˆä¾‹ï¼Œé™¤éæ˜¾å¼è¦æ±‚
    if request.mode != "selection_polish":
        sources_str = get_relevant_laws_formatted(last_user_msg)
        if sources_str:
            context_text = f"""
### ğŸ“š æƒå¨å‚è€ƒèµ„æ–™åº“
ä»¥ä¸‹æ˜¯ç³»ç»Ÿä¸ºæ‚¨æ£€ç´¢åˆ°çš„ç›¸å…³æ³•å¾‹ä¾æ®ä¸çœŸå®åˆ¤ä¾‹ã€‚è¯·ä»”ç»†é˜…è¯»ï¼Œå¹¶åœ¨æ’°å†™æ—¶**å¼•ç”¨**è¿™äº›èµ„æ–™ï¼ˆä½¿ç”¨ [1], [2] è§’æ ‡ï¼‰ã€‚

{sources_str}
"""
        else:
            context_text = "ï¼ˆæœ¬æ¬¡æœªæ£€ç´¢åˆ°é«˜åº¦ç›¸å…³çš„ç‰¹å®šæ¡ˆä¾‹ï¼Œè¯·ä¾æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹åŠé€šç”¨å®åŠ¡ç»éªŒæ’°å†™ï¼‰"

    # --- 2. Prompt æ„å»º (Role + Task + Constraints) ---
    base_role = "ä½ æ˜¯ç”± LawLens å¼€å‘çš„ä¸­å›½é¡¶å°–æ³•å¾‹ AI åŠ©æ‰‹ã€‚ä½ çš„å›ç­”å¿…é¡»å…·å¤‡çº¢åœˆå¾‹æ‰€é«˜çº§åˆä¼™äººçš„æ°´å‡†ï¼šä¸¥è°¨ã€çŠ€åˆ©ã€é€»è¾‘é—­ç¯ã€‚"
    
    # é€šç”¨æ’ç‰ˆè¦æ±‚ (é€‚é… Tiptap ç¼–è¾‘å™¨)
    html_hint = """
    ã€æ’ç‰ˆè¦æ±‚ã€‘
    1. **å¿…é¡»è¾“å‡º HTML æ ‡ç­¾**ï¼šä½¿ç”¨ <h3> è¡¨ç¤ºå°æ ‡é¢˜ï¼Œ<b> è¡¨ç¤ºé‡ç‚¹ï¼Œ<p> è¡¨ç¤ºæ®µè½ï¼Œ<ul>/<li> è¡¨ç¤ºåˆ—è¡¨ã€‚
    2. **ä¸¥ç¦ä½¿ç”¨ Markdown**ï¼šä¸è¦ç”¨ # æˆ– **ï¼Œä¹Ÿä¸è¦è¾“å‡º ```html ä»£ç å—ã€‚
    3. **å¼•ç”¨æ ‡æ³¨**ï¼šåœ¨å¼•ç”¨äº†å‚è€ƒèµ„æ–™çš„è§‚ç‚¹æˆ–æ³•æ¡æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«æ ‡æ³¨ [1] ç­‰æ¥æºè§’æ ‡ã€‚
    """

    system_instruction = ""

    if request.mode == "selection_polish":
        # Case A: å±€éƒ¨æ¶¦è‰² (å¾®è§‚æ“ä½œ)
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘ç”¨æˆ·é€‰ä¸­äº†ä¸€æ®µæ–‡æœ¬ï¼Œè¯·å¯¹å…¶è¿›è¡Œã€æ³•è¨€æ³•è¯­é‡æ„ã€‘ã€‚
        
        ã€åŸæ–‡ã€‘ï¼š"{request.selection}"
        ã€æŒ‡ä»¤ã€‘ï¼š"{last_user_msg}"
        
        ã€è¦æ±‚ã€‘ï¼š
        1. ä»…è¾“å‡ºä¿®æ”¹åçš„æ–‡æœ¬ï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–å¯’æš„ã€‚
        2. ä¿®æ­£å£è¯­åŒ–è¡¨è¾¾ï¼ˆå¦‚â€œé’±æ²¡ç»™â€->â€œæœªå±¥è¡Œä»˜æ¬¾ä¹‰åŠ¡â€ï¼‰ã€‚
        3. ä¿æŒ HTML æ ¼å¼ã€‚
        """
    
    elif request.mode == "polish":
        # Case B: å…¨æ–‡æ¶¦è‰² (å®è§‚æ“ä½œ)
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘è¯·åƒä¸€ä½ä¸¥å‰çš„å¾‹æ‰€åˆä¼™äººä¸€æ ·ï¼Œå®¡æŸ¥å¹¶æ¶¦è‰²æ•´ç¯‡æ–‡æ¡£ã€‚
        
        ã€å¾…å®¡æ–‡æ¡£ã€‘ï¼š
        '''{request.current_doc}'''
        
        {context_text}
        
        ã€è¾“å‡ºç»“æ„ã€‘ï¼š
        1. <h3>å®¡æŸ¥æ„è§</h3>ï¼šç”¨ä¸€æ®µè¯æŒ‡å‡ºæ–‡æ¡£çš„ä¸»è¦æ³•å¾‹é£é™©ç‚¹æˆ–é€»è¾‘æ¼æ´ã€‚
        2. <h3>ä¿®è®¢åå…¨æ–‡</h3>ï¼šè¾“å‡ºå®Œæ•´çš„ã€ä¼˜åŒ–åçš„æ–‡æ¡£å†…å®¹ã€‚é‡ç‚¹ä¿®æ”¹å¤„è¯·ç”¨ <b>åŠ ç²—</b> æ ‡å‡ºã€‚
        
        {html_hint}
        """
        
    else: 
        # Case C: ä»é›¶ç”Ÿæˆ (Draft) - åŠ å…¥æ€ç»´é“¾
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œå‚è€ƒç±»ä¼¼æ¡ˆä¾‹çš„å†™æ³•ï¼Œä»é›¶èµ·è‰æ³•å¾‹æ–‡ä¹¦ã€‚
        
        {context_text}
        
        ã€è¾“å‡ºç»“æ„ã€‘ï¼š
        1. **æ€ç»´é“¾ï¼ˆThinking Chainï¼‰**ï¼šåœ¨æ­£å¼èµ·è‰å‰ï¼Œå…ˆè¾“å‡ºä¸€æ®µ `<blockquote>`ï¼Œç®€è¦åˆ†ææ¡ˆç”±ã€æ ¸å¿ƒæ³•æ¡å’Œè¯‰è®¼ç­–ç•¥ã€‚
        2. **æ­£å¼æ–‡ä¹¦**ï¼šéšåè¾“å‡ºå®Œæ•´çš„æ³•å¾‹æ–‡ä¹¦ã€‚ç»“æ„å¿…é¡»å®Œæ•´ï¼ˆé¦–éƒ¨ã€æ­£æ–‡ã€å°¾éƒ¨ï¼‰ã€‚
        
        {html_hint}
        """

    # --- 3. æ¶ˆæ¯å†å²æ„å»º ---
    llm_messages = [{"role": "system", "content": system_instruction}]
    
    if request.mode == "selection_polish":
        llm_messages.append({"role": "user", "content": last_user_msg})
    else:
        # è¿‡æ»¤æ‰ä¹‹å‰çš„ system promptï¼Œä¿ç•™å¯¹è¯å†å²
        clean_history = [m.dict() for m in request.messages if m.role != 'system']
        llm_messages.extend(clean_history)

    # --- 4. æµå¼ç”Ÿæˆå™¨ ---
    async def generate_stream():
        try:
            # æ¨¡æ‹Ÿâ€œæ€è€ƒä¸­â€çŠ¶æ€ (ä»…åœ¨èµ·è‰æ¨¡å¼)
            if request.mode == "draft":
                yield "<blockquote>âš–ï¸ æ­£åœ¨æ£€ç´¢åˆ¤ä¾‹åº“... æ„å»ºæ³•å¾‹é€»è¾‘é“¾æ¡...</blockquote>"
            
            stream = client.chat.completions.create(
                model="Qwen/Qwen2.5-32B-Instruct", 
                messages=llm_messages,
                stream=True, 
                temperature=0.4, # ç¨ä½æ¸©åº¦ï¼Œä¿è¯æ³•å¾‹ä¸¥è°¨æ€§
                max_tokens=4000 
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            error_msg = f"<p style='color:red'>[AI ç”Ÿæˆä¸­æ–­: {str(e)}]</p>"
            print(f"âŒ AI Error: {e}")
            yield error_msg

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)