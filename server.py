import os
import uvicorn
import time
import json
import mammoth
import io
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from supabase import create_client, Client
from openai import OpenAI
from typing import List, Optional

# ===========================
# 1. é…ç½®ä¸åˆå§‹åŒ–
# ===========================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")

# âœ¨ æ¨¡å‹é€‰æ‹©ï¼šä½¿ç”¨ Qwen 2.5 ç³»åˆ—æœ€å¼ºç‰ˆæœ¬ (72B)
MODEL_NAME = "Qwen/Qwen2.5-72B-Instruct"

supabase: Optional[Client] = None
client: Optional[OpenAI] = None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
def startup_event():
    global supabase, client
    if not all([SUPABASE_URL, SUPABASE_KEY, SILICONFLOW_API_KEY]):
        print("âŒ é”™è¯¯ï¼šæ ¸å¿ƒç¯å¢ƒå˜é‡ç¼ºå¤±")
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="https://api.siliconflow.cn/v1"
        )
        print(f"âœ… LawLens æ™ºèƒ½å¼•æ“å·²å¯åŠ¨ (æ¨¡å‹: {MODEL_NAME} | å…¨ä¸­æ–‡æ¨¡å¼)")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

# ===========================
# 2. æ•°æ®æ¨¡å‹
# ===========================
class ChatMessage(BaseModel):
    role: str
    content: str

class AnalyzeRequest(BaseModel):
    messages: List[ChatMessage]
    current_doc: str = ""
    selection: Optional[str] = "" 
    mode: str = "draft" # draft | polish | selection_polish | risk_score | chat_doc
    user_id: Optional[str] = None 

class DocumentSave(BaseModel):
    title: str
    content: str
    user_id: Optional[str] = None

class MemoryCreate(BaseModel):
    user_id: str
    content: str
    type: str = "preference"

# ===========================
# 3. ğŸ§  Memory Manager
# ===========================
class MemoryManager:
    @staticmethod
    def add_memory(user_id: str, content: str, m_type: str = "preference"):
        if not client or not supabase: return False
        try:
            resp = client.embeddings.create(model="BAAI/bge-m3", input=content)
            vec = resp.data[0].embedding
            supabase.table("agent_memories").insert({
                "user_id": user_id, "content": content, "memory_type": m_type, "embedding": vec
            }).execute()
            return True
        except Exception: return False

    @staticmethod
    def retrieve_memories(user_id: str, query: str) -> str:
        if not client or not supabase or not user_id: return ""
        try:
            resp = client.embeddings.create(model="BAAI/bge-m3", input=query)
            vec = resp.data[0].embedding
            rpc_resp = supabase.rpc("match_memories", {
                "query_embedding": vec, "match_threshold": 0.5, "match_count": 3, "p_user_id": user_id
            }).execute()
            if not rpc_resp.data: return ""
            return "\n".join([f"- {m['content']}" for m in rpc_resp.data])
        except Exception: return ""

# ===========================
# 4. è¾…åŠ©æ¥å£
# ===========================
@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        content = await file.read()
        result = mammoth.convert_to_html(io.BytesIO(content))
        return {"status": "success", "content": result.value}
    except Exception as e:
        return {"status": "error", "msg": "æ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·ç¡®ä¿æ˜¯ .docx æ–‡ä»¶"}

@app.post("/api/memory")
async def create_memory(mem: MemoryCreate):
    success = MemoryManager.add_memory(mem.user_id, mem.content, mem.type)
    return {"status": "success" if success else "error"}

@app.post("/api/save")
async def save_document(doc: DocumentSave):
    if not supabase: return {"status": "error", "msg": "DBæœªè¿æ¥"}
    try:
        raw_text = doc.content.replace('<', '').replace('>', '')[:20]
        title = doc.title if doc.title and doc.title != "æœªå‘½åæ³•å¾‹æ–‡ä¹¦" else f"{raw_text}..."
        supabase.table("documents").insert({"title": title, "content": doc.content, "user_id": doc.user_id}).execute()
        return {"status": "success"}
    except Exception as e:
        return {"status": "error", "msg": str(e)}

@app.get("/api/history")
async def get_history(user_id: Optional[str] = None):
    if not supabase: return []
    try:
        query = supabase.table("documents").select("*").order("created_at", desc=True).limit(20)
        if user_id: query = query.eq("user_id", user_id)
        else: query = query.is_("user_id", "null")
        return query.execute().data
    except Exception: return []

# ===========================
# 5. æ ¸å¿ƒ AI é€»è¾‘ (å…¨æ±‰åŒ–)
# ===========================

def get_rag_context(query: str):
    if not client or not supabase: return ""
    try:
        resp = client.embeddings.create(model="BAAI/bge-m3", input=query)
        vec = resp.data[0].embedding
        rpc_resp = supabase.rpc("match_documents", {
            "query_embedding": vec, "match_threshold": 0.45, "match_count": 3 
        }).execute()
        
        if not rpc_resp.data: return ""
        formatted = ""
        for i, doc in enumerate(rpc_resp.data):
            snippet = doc['content'][:500].replace('\n', ' ')
            formatted += f"ã€å‚è€ƒèµ„æ–™ {i+1}ã€‘\n{snippet}...\n"
        return formatted
    except Exception: return ""

@app.post("/api/analyze")
async def analyze(request: AnalyzeRequest):
    """æ ¸å¿ƒ AI æ¥å£"""
    
    # --- P2: é£é™©è¯„åˆ† (JSON) ---
    if request.mode == "risk_score":
        try:
            prompt = f"""
            ä½ æ˜¯ä¸€åèµ„æ·±æ³•å¾‹é£æ§ä¸“å®¶ã€‚è¯·é˜…è¯»ä»¥ä¸‹æ–‡ä¹¦ï¼Œä»å››ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆ0-100ï¼‰ã€‚
            ã€å¾…å®¡æ–‡ä¹¦ã€‘{request.current_doc[:3000]}
            ã€è¾“å‡ºè¦æ±‚ã€‘ä»…è¾“å‡ºæ ‡å‡† JSONï¼š
            {{
                "total_score": 85,
                "summary": "ä¸€å¥è¯ä¸­æ–‡ç®€è¯„",
                "dimensions": [
                    {{ "subject": "åˆè§„æ€§", "A": 90, "fullMark": 100 }},
                    {{ "subject": "æƒç›Šä¿æŠ¤", "A": 75, "fullMark": 100 }},
                    {{ "subject": "å®Œæ•´æ€§", "A": 85, "fullMark": 100 }},
                    {{ "subject": "æ–‡æœ¬è§„èŒƒ", "A": 95, "fullMark": 100 }}
                ]
            }}
            """
            completion = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1, response_format={"type": "json_object"}
            )
            result = json.loads(completion.choices[0].message.content)
            return JSONResponse(result)
        except Exception:
            return JSONResponse({"error": "åˆ†æå¤±è´¥"}, status=500)

    # --- å¸¸è§„æµå¼æ¨¡å¼ ---
    last_user_msg = request.selection if request.mode == "selection_polish" else request.messages[-1].content
    user_id = request.user_id
    
    # 1. RAG
    rag_context = ""
    found_cases = False
    if request.mode != "selection_polish" and request.mode != "chat_doc":
        rag_context = get_rag_context(last_user_msg)
        if rag_context: found_cases = True

    # 2. è®°å¿†
    memory_context = ""
    if user_id:
        memory_context = MemoryManager.retrieve_memories(user_id, last_user_msg)

    # 3. æ„é€ ä¸­æ–‡ Prompt
    memory_section = f"ã€âš ï¸ ç”¨æˆ·åå¥½è®°å¿†ã€‘\nè¯·ä¸¥æ ¼éµå®ˆï¼š{memory_context}\n" if memory_context else ""
    rag_section = f"ã€ğŸ“š æ³•å¾‹æ•°æ®åº“ã€‘\n{rag_context}\n" if rag_context else "ï¼ˆä½¿ç”¨é€šç”¨æ³•å¾‹çŸ¥è¯†ï¼‰"

    base_role = "ä½ æ˜¯ç”± LawLens å¼€å‘çš„ä¸­å›½é¡¶å°–æ³•å¾‹ AI åŠ©æ‰‹ã€‚ä½ çš„å›ç­”å¿…é¡»ä¸“ä¸šã€ä¸¥è°¨ã€ç¬¦åˆä¸­å›½æ³•å¾‹è§„èŒƒã€‚"
    html_hint = "ä½¿ç”¨ HTML æ ‡ç­¾æ’ç‰ˆ (<h3>, <b>, <ul>, <blockquote>)ï¼Œç¦æ­¢ Markdownã€‚"

    system_instruction = ""

    if request.mode == "draft":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘æ ¹æ®ç”¨æˆ·éœ€æ±‚èµ·è‰æ³•å¾‹æ–‡ä¹¦ã€‚
        {memory_section}
        {rag_section}
        {html_hint}
        ã€è¾“å‡ºç»“æ„ã€‘
        1. **åˆ†ææŠ¥å‘Š** (<blockquote>):
           - **æ ¸å¿ƒäº‰è®®ç‚¹**: åˆ†ææ³•å¾‹å…³ç³»ã€‚
           - **æ³•å¾‹ä¾æ®**: å¼•ç”¨ç›¸å…³æ³•æ¡æˆ–æ¡ˆä¾‹è§‚ç‚¹ã€‚
           - **èµ·è‰ç­–ç•¥**: è¯´æ˜é‡ç‚¹æ¡æ¬¾çš„è®¾è®¡æ€è·¯ã€‚
        2. **æ­£å¼æ–‡ä¹¦**: å®Œæ•´çš„åˆåŒæˆ–å‡½ä»¶å†…å®¹ã€‚
        """
    elif request.mode == "polish":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘å®¡æŸ¥å¹¶æ¶¦è‰²æ³•å¾‹æ–‡ä¹¦ã€‚
        {memory_section}
        ã€å¾…å®¡æ–‡æ¡£ã€‘'''{request.current_doc}'''
        {rag_section}
        {html_hint}
        ã€è¾“å‡ºç»“æ„ã€‘
        1. **å®¡æŸ¥æ„è§** (<blockquote>):
           - **é£é™©æç¤º**: æŒ‡å‡ºæ³•å¾‹æ¼æ´ã€‚
           - **ä¿®æ”¹ä¾æ®**: è§£é‡Šä¸ºä»€ä¹ˆè¦æ”¹ã€‚
        2. **ä¿®è®¢åå…¨æ–‡**: è¾“å‡ºå®Œæ•´æ–‡æœ¬ï¼Œç”¨ <b>åŠ ç²—</b> æ ‡è®°ä¿®æ”¹å¤„ã€‚
        """
    elif request.mode == "chat_doc":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘åŸºäºå½“å‰æ–‡æ¡£å›ç­”é—®é¢˜ã€‚
        ã€æ–‡æ¡£å†…å®¹ã€‘'''{request.current_doc[:10000]}'''
        ã€ç”¨æˆ·é—®é¢˜ã€‘"{last_user_msg}"
        ã€è¦æ±‚ã€‘ç­”æ¡ˆå¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹ï¼Œä¸è¦ç¼–é€ ã€‚å¼•ç”¨åŸæ–‡æ—¶è¯·åŠ ç²—ã€‚
        """
    else: # selection_polish
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘å¾®è°ƒé€‰ä¸­çš„æ–‡æœ¬ï¼Œä½¿å…¶æ›´ç¬¦åˆæ³•è¨€æ³•è¯­ã€‚
        {memory_section}
        ã€åŸæ–‡ã€‘"{request.selection}"
        ã€æŒ‡ä»¤ã€‘"{last_user_msg}"
        ã€è¦æ±‚ã€‘ä»…è¾“å‡ºä¿®æ”¹åçš„æ–‡æœ¬ï¼Œä¸è¦åºŸè¯ã€‚
        """

    messages = [{"role": "system", "content": system_instruction}]
    if request.mode != "selection_polish":
        history = [m.dict() for m in request.messages if m.role != "system"]
        messages.extend(history)
    else:
        messages.append({"role": "user", "content": last_user_msg})

    async def generate_stream():
        try:
            # A. è¿›åº¦æ¡ (å…¨ä¸­æ–‡)
            if request.mode != "selection_polish":
                status_rag = f"âœ… å·²åŒ¹é… {rag_context.count('ã€å‚è€ƒèµ„æ–™')} ä¸ªç›¸å…³æ¡ˆä¾‹" if found_cases else "âš ï¸ é€šç”¨æ³•å¾‹æ¨¡å¼"
                status_mem = "âœ… å‘½ä¸­ç”¨æˆ·åå¥½" if memory_context else "æ— ç‰¹å®šåå¥½"
                
                status_html = f"""
                <div style="background:#f8fafc; padding:12px; border-radius:8px; border:1px solid #e2e8f0; margin-bottom:16px; font-size:13px; color:#475569;">
                    <div style="display:flex; align-items:center; gap:8px; margin-bottom:8px;">
                        <span style="display:inline-block; width:8px; height:8px; background:#2563eb; border-radius:50%;"></span>
                        <b>AI æ³•å¾‹å¼•æ“è¿è¡Œä¸­...</b>
                    </div>
                    <ul style="margin:0; padding-left:20px; line-height: 1.6;">
                        <li>æ­£åœ¨åˆ†ææ¡ˆæƒ…ï¼š{last_user_msg[:10]}...</li>
                        <li>æ£€ç´¢æ•°æ®åº“ï¼š{status_rag}</li>
                        <li>æ£€ç´¢è®°å¿†åº“ï¼š{status_mem}</li>
                        <li>æ„å»ºé€»è¾‘é“¾ï¼šäº‹å®è®¤å®š -> æ³•æ¡åŒ¹é… -> æ–‡ä¹¦ç”Ÿæˆ</li>
                    </ul>
                </div>
                """
                yield status_html
                time.sleep(0.5)

            stream = client.chat.completions.create(
                model=MODEL_NAME, # 72B
                messages=messages,
                stream=True, 
                temperature=0.4,
                max_tokens=4000 
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"<p style='color:red'>AI æœåŠ¡å“åº”é”™è¯¯: {str(e)}</p>"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)