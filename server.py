import os
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse 
from pydantic import BaseModel
from supabase import create_client, Client
from openai import OpenAI
from typing import List, Optional
import json

# 1. ç¯å¢ƒå˜é‡
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")

# 2. å…¨å±€å®¢æˆ·ç«¯
supabase: Optional[Client] = None
client: Optional[OpenAI] = None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
def startup_event():
    global supabase, client
    if not all([SUPABASE_URL, SUPABASE_KEY, SILICONFLOW_API_KEY]):
        print("âŒ é”™è¯¯ï¼šæ ¸å¿ƒç¯å¢ƒå˜é‡ç¼ºå¤±")
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="https://api.siliconflow.cn/v1"
        )
        print("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (SiliconFlow / Qwen)")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

# --- Pydantic æ¨¡å‹ ---
class ChatMessage(BaseModel):
    role: str
    content: str

class AnalyzeRequest(BaseModel):
    messages: List[ChatMessage]
    current_doc: str = ""
    mode: str = "draft"

class DocumentSave(BaseModel):
    title: str
    content: str
    user_id: Optional[str] = None

# --- è¾…åŠ©æ¥å£ (Save / History) ---
@app.post("/api/save")
async def save_document(doc: DocumentSave):
    if not supabase: return {"status": "error", "msg": "DBæœªè¿æ¥"}
    try:
        data = {"title": doc.title, "content": doc.content, "user_id": doc.user_id}
        supabase.table("documents").insert(data).execute()
        return {"status": "success"}
    except Exception as e:
        print(f"Save error: {e}")
        return {"status": "error", "msg": str(e)}

@app.get("/api/history")
async def get_history(user_id: Optional[str] = None):
    if not supabase: return []
    try:
        query = supabase.table("documents").select("*").order("created_at", desc=True).limit(20)
        if user_id: query = query.eq("user_id", user_id)
        else: query = query.is_("user_id", "null")
        res = query.execute()
        return res.data
    except Exception as e:
        print(f"History error: {e}")
        return []

# --- æ ¸å¿ƒå‡çº§ï¼šRAG æ£€ç´¢ä¸æµå¼ AI åˆ†æ ---

def get_relevant_laws(query: str):
    if not client or not supabase: return []
    try:
        # 1. ç”Ÿæˆå‘é‡ (BAAI/bge-m3)
        response = client.embeddings.create(model="BAAI/bge-m3", input=query)
        query_vector = response.data[0].embedding
        
        # 2. æ•°æ®åº“æŸ¥è¯¢ (ç¡®ä¿ SQL match_documents é€‚é… 1024 ç»´åº¦)
        rpc_response = supabase.rpc("match_documents", {
            "query_embedding": query_vector,
            "match_threshold": 0.35,
            "match_count": 5
        }).execute()
        return rpc_response.data
    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        return []

@app.post("/api/analyze")
async def analyze(request: AnalyzeRequest):
    last_user_msg = request.messages[-1].content
    print(f"ğŸ” æµå¼è¯·æ±‚: {last_user_msg[:20]}... æ¨¡å¼: {request.mode}")
    
    # 1. RAG æ£€ç´¢
    relevant_docs = get_relevant_laws(last_user_msg)
    
    context_text = ""
    if relevant_docs:
        context_text = "ã€æƒå¨æ³•å¾‹ä¾æ®åº“ï¼ˆå¿…é¡»ä¼˜å…ˆå¼•ç”¨ï¼‰ã€‘\n" + "\n".join(
            [f"ä¾æ®{i+1}:ã€Š{d['law_name']}ã€‹\næ¡æ¬¾å†…å®¹:{d['content'][:400]}" 
             for i, d in enumerate(relevant_docs)]
        )
    else:
        context_text = "ï¼ˆæœªæ£€ç´¢åˆ°ç‰¹å®šåº“å†…æ¡ˆä¾‹ï¼Œè¯·ä¸¥æ ¼ä¾æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹åŠç›¸å…³å¸æ³•è§£é‡Šï¼‰"

    # 2. æ„å»ºè¶…çº§ System Prompt (äººè®¾ + æ ¼å¼æ§åˆ¶)
    
    base_role = """
    ä½ æ˜¯ä¸€åæ‹¥æœ‰ 20 å¹´ç»éªŒçš„ä¸­å›½çº¢åœˆå¾‹æ‰€é«˜çº§åˆä¼™äººï¼Œä¸“ç²¾äºæ°‘å•†äº‹è¯‰è®¼æ–‡ä¹¦ã€‚
    ä½ çš„æ–‡ä¹¦é£æ ¼å¿…é¡»ï¼šç»“æ„ä¸¥è°¨ã€é€»è¾‘ç¼œå¯†ã€ç”¨è¯æå…¶ä¸“ä¸šï¼ˆæ³•è¨€æ³•è¯­ï¼‰ã€‚
    """

    format_instruction = """
    ã€é‡è¦æ ¼å¼è¦æ±‚ã€‘
    å‰ç«¯ä½¿ç”¨å¯Œæ–‡æœ¬ç¼–è¾‘å™¨ï¼Œè¯·ç›´æ¥è¾“å‡º HTML æ ¼å¼çš„ä»£ç ï¼Œä¸è¦ä½¿ç”¨ Markdownã€‚
    1. ä½¿ç”¨ <p> åŒ…è£¹æ®µè½ã€‚
    2. ä½¿ç”¨ <b> æˆ– <strong> åŠ ç²—é‡è¦çš„å°æ ‡é¢˜ï¼ˆå¦‚â€œäº‹å®ä¸ç†ç”±â€ã€â€œè¯‰è®¼è¯·æ±‚â€ï¼‰ã€‚
    3. ä½¿ç”¨ <br> è¿›è¡Œæ¢è¡Œã€‚
    4. ä¸¥ç¦ä½¿ç”¨ ```html ä»£ç å—åŒ…è£¹ï¼Œç›´æ¥è¾“å‡ºå†…å®¹å³å¯ã€‚
    """

    if request.mode == "polish":
        system_instruction = f"""
        {base_role}
        
        ã€ä»»åŠ¡ç›®æ ‡ã€‘
        å¯¹ç”¨æˆ·æä¾›çš„æ³•å¾‹æ–‡ä¹¦åˆç¨¿è¿›è¡Œä¸“ä¸šçº§æ¶¦è‰²ã€‚
        
        ã€åŸå§‹æ–‡æ¡£å†…å®¹ã€‘
        '''
        {request.current_doc}
        '''

        ã€ä¿®æ”¹è¦æ±‚ã€‘
        1. **æœ¯è¯­ä¸“ä¸šåŒ–**ï¼šå°†å£è¯­è¡¨è¾¾è½¬åŒ–ä¸ºæ ‡å‡†æ³•è¨€æ³•è¯­ï¼ˆä¾‹å¦‚ï¼šå°†â€œæƒ³è¦é’±â€æ”¹ä¸ºâ€œè¯‰è¯·æ”¯ä»˜â€ï¼›å°†â€œè¯´è¯ä¸ç®—æ•°â€æ”¹ä¸ºâ€œæ„æˆæ ¹æœ¬è¿çº¦â€ï¼‰ã€‚
        2. **é€»è¾‘ä¸¥å¯†æ€§**ï¼šæ£€æŸ¥å› æœå…³ç³»ï¼Œä½¿ç”¨â€œé‰´äº...â€ã€â€œç»¼ä¸Šæ‰€è¿°...â€ç­‰è¿æ¥è¯å¢å¼ºé€»è¾‘é“¾ã€‚
        3. **å¼•ç”¨è§„èŒƒåŒ–**ï¼šå‚è€ƒä¸‹æ–¹çš„ã€æƒå¨æ³•å¾‹ä¾æ®åº“ã€‘ï¼Œå¯¹æ–‡ä¸­çš„æ³•æ¡å¼•ç”¨è¿›è¡Œæ ¸å¯¹æˆ–è¡¥å……ã€‚
        4. **HTMLæ’ç‰ˆ**ï¼šé‡ç‚¹å†…å®¹ï¼ˆå¦‚é‡‘é¢ã€å…³é”®æ³•æ¡ï¼‰è¯·ä½¿ç”¨ <b> åŠ ç²—ã€‚
        
        {format_instruction}
        {context_text}
        """
    else: # draft mode
        system_instruction = f"""
        {base_role}
        
        ã€ä»»åŠ¡ç›®æ ‡ã€‘
        æ ¹æ®ç”¨æˆ·æä¾›çš„æ¡ˆæƒ…æè¿°ï¼Œä»é›¶èµ·è‰ä¸€ä»½ç»“æ„ä¸¥è°¨ã€æ”»é˜²å…¼å¤‡çš„æ³•å¾‹æ–‡ä¹¦ã€‚
        
        ã€èµ·è‰æ ‡å‡†ã€‘
        1. **ç»“æ„å®Œå¤‡**ï¼šå¿…é¡»åŒ…å«é¦–éƒ¨ï¼ˆåŸè¢«å‘Šä¿¡æ¯ï¼‰ã€è¯‰è®¼è¯·æ±‚ã€äº‹å®ä¸ç†ç”±ã€å°¾éƒ¨ï¼ˆè‡´è°¢ã€å…·çŠ¶äººã€æ—¥æœŸï¼‰å››å¤§æ¿å—ã€‚
        2. **äº‹å®é™ˆè¿°**ï¼šé‡‡ç”¨â€œæ—¶é—´è½´+æ³•å¾‹äº‹å®â€çš„å™è¿°æ–¹å¼ï¼Œå†·é™ã€å®¢è§‚ã€æœ‰åŠ›ã€‚
        3. **æ³•å¾‹é€‚ç”¨**ï¼šå¿…é¡»åœ¨â€œç†ç”±â€éƒ¨åˆ†æ˜¾å¼å¼•ç”¨ä¸‹æ–¹çš„ã€æƒå¨æ³•å¾‹ä¾æ®åº“ã€‘ã€‚å¼•ç”¨æ ¼å¼ä¸ºï¼šâ€œæ ¹æ®ã€ŠXXæ³•ã€‹ç¬¬XXæ¡ä¹‹è§„å®š...â€ã€‚
        4. **HTMLæ’ç‰ˆ**ï¼š
           - å°æ ‡é¢˜ï¼ˆå¦‚ã€è¯‰è®¼è¯·æ±‚ã€‘ï¼‰è¯·ä½¿ç”¨ <b> åŠ ç²—ã€‚
           - å…³é”®é‡‘é¢è¯·ä½¿ç”¨ <b> åŠ ç²—ã€‚
           - æ®µè½ä¹‹é—´ä¿æŒé€‚å½“é—´è·ã€‚

        {format_instruction}
        {context_text}
        """

    llm_messages = [{"role": "system", "content": system_instruction}]
    # åªå–æœ€è¿‘å‡ æ¡æ¶ˆæ¯ï¼Œé¿å… System Prompt è¢«æ·¹æ²¡
    recent_history = request.messages[-3:] if len(request.messages) > 3 else request.messages
    llm_messages.extend([m.dict() for m in recent_history if m.role != 'system'])

    # 3. å®šä¹‰ç”Ÿæˆå™¨ (Generator)
    async def generate_stream():
        try:
            stream = client.chat.completions.create(
                model="Qwen/Qwen2.5-32B-Instruct", 
                messages=llm_messages,
                stream=True, 
                temperature=0.7,
                max_tokens=2500 # å¢åŠ é•¿åº¦ä»¥é˜²æˆªæ–­
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"<p style='color:red'>[System Error: {str(e)}]</p>"

    # 4. è¿”å›æµå¼å“åº”
    return StreamingResponse(generate_stream(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)