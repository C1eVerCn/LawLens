import os
import uvicorn
import time
import json
import mammoth
import io
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from supabase import create_client, Client
from openai import OpenAI
from typing import List, Optional

# ===========================
# 1. é…ç½®ä¸åˆå§‹åŒ–
# ===========================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")

supabase: Optional[Client] = None
client: Optional[OpenAI] = None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
def startup_event():
    global supabase, client
    if not all([SUPABASE_URL, SUPABASE_KEY, SILICONFLOW_API_KEY]):
        print("âŒ é”™è¯¯ï¼šæ ¸å¿ƒç¯å¢ƒå˜é‡ç¼ºå¤±")
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="https://api.siliconflow.cn/v1"
        )
        print("âœ… LawLens æ™ºèƒ½å¼•æ“å·²å¯åŠ¨ (Memory + Deep RAG + Risk + Upload)")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

# ===========================
# 2. æ•°æ®æ¨¡å‹
# ===========================
class ChatMessage(BaseModel):
    role: str
    content: str

class AnalyzeRequest(BaseModel):
    messages: List[ChatMessage]
    current_doc: str = ""
    selection: Optional[str] = "" 
    mode: str = "draft" # draft | polish | selection_polish | risk_score | chat_doc
    user_id: Optional[str] = None 

class DocumentSave(BaseModel):
    title: str
    content: str
    user_id: Optional[str] = None

class MemoryCreate(BaseModel):
    user_id: str
    content: str
    type: str = "preference"

# ===========================
# 3. ğŸ§  Memory Manager (è®°å¿†ç®¡ç†æ¨¡å—)
# ===========================
class MemoryManager:
    @staticmethod
    def add_memory(user_id: str, content: str, m_type: str = "preference"):
        if not client or not supabase: return False
        try:
            resp = client.embeddings.create(model="BAAI/bge-m3", input=content)
            vec = resp.data[0].embedding
            supabase.table("agent_memories").insert({
                "user_id": user_id, "content": content, "memory_type": m_type, "embedding": vec
            }).execute()
            print(f"ğŸ§  [Memory] å·²è®°ä½: {content}")
            return True
        except Exception as e:
            print(f"âŒ Memory Write Error: {e}")
            return False

    @staticmethod
    def retrieve_memories(user_id: str, query: str) -> str:
        if not client or not supabase or not user_id: return ""
        try:
            resp = client.embeddings.create(model="BAAI/bge-m3", input=query)
            vec = resp.data[0].embedding
            rpc_resp = supabase.rpc("match_memories", {
                "query_embedding": vec, "match_threshold": 0.5, "match_count": 3, "p_user_id": user_id
            }).execute()
            if not rpc_resp.data: return ""
            return "\n".join([f"- {m['content']}" for m in rpc_resp.data])
        except Exception as e:
            print(f"âŒ Memory Read Error: {e}")
            return ""

# ===========================
# 4. è¾…åŠ©æ¥å£ (Wordè§£æ + å†å² + ä¿å­˜)
# ===========================

# âœ¨ P0: Word ä¸Šä¼ è§£ææ¥å£
@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        content = await file.read()
        # ä½¿ç”¨ mammoth å°† docx è½¬æ¢ä¸º HTML
        result = mammoth.convert_to_html(io.BytesIO(content))
        html = result.value
        return {"status": "success", "content": html}
    except Exception as e:
        print(f"Upload failed: {e}")
        return {"status": "error", "msg": "æ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·ç¡®ä¿æ˜¯ .docx æ–‡ä»¶"}

@app.post("/api/memory")
async def create_memory(mem: MemoryCreate):
    success = MemoryManager.add_memory(mem.user_id, mem.content, mem.type)
    return {"status": "success" if success else "error"}

@app.post("/api/save")
async def save_document(doc: DocumentSave):
    if not supabase: return {"status": "error", "msg": "DBæœªè¿æ¥"}
    try:
        # æ™ºèƒ½æˆªå–æ ‡é¢˜
        raw_text = doc.content.replace('<', '').replace('>', '')[:20]
        title = doc.title if doc.title and doc.title != "æœªå‘½åæ³•å¾‹æ–‡ä¹¦" else f"{raw_text}..."
        
        data = {"title": title, "content": doc.content, "user_id": doc.user_id}
        supabase.table("documents").insert(data).execute()
        return {"status": "success"}
    except Exception as e:
        print(f"Save error: {e}")
        return {"status": "error", "msg": str(e)}

@app.get("/api/history")
async def get_history(user_id: Optional[str] = None):
    if not supabase: return []
    try:
        query = supabase.table("documents").select("*").order("created_at", desc=True).limit(20)
        if user_id: query = query.eq("user_id", user_id)
        else: query = query.is_("user_id", "null")
        res = query.execute()
        return res.data
    except Exception as e:
        print(f"History error: {e}")
        return []

# ===========================
# 5. æ ¸å¿ƒ AI é€»è¾‘ (RAG + é£é™©è¯„åˆ†)
# ===========================

def get_relevant_laws_formatted(query: str):
    """Deep RAG æ£€ç´¢"""
    if not client or not supabase: return ""
    try:
        print(f"ğŸ” [RAG] æ£€ç´¢: {query[:15]}...")
        response = client.embeddings.create(model="BAAI/bge-m3", input=query)
        query_vector = response.data[0].embedding
        rpc_response = supabase.rpc("match_documents", {
            "query_embedding": query_vector, "match_threshold": 0.45, "match_count": 4 
        }).execute()
        
        data = rpc_response.data
        if not data: return ""

        formatted_sources = []
        for idx, doc in enumerate(data):
            meta = doc.get('metadata', {}) or {}
            source_name = doc.get('law_name') or meta.get('source') or "æ³•å¾‹æ•°æ®åº“"
            content_snippet = doc['content'][:500].replace("\n", " ")
            block = f"[å‚è€ƒèµ„æ–™ {idx + 1}] æ¥æºï¼š{source_name}\nå†…å®¹ï¼š{content_snippet}..."
            formatted_sources.append(block)
            
        return "\n\n".join(formatted_sources)
    except Exception as e:
        print(f"âŒ RAG Error: {e}")
        return ""

@app.post("/api/analyze")
async def analyze(request: AnalyzeRequest):
    """æ ¸å¿ƒ AI æ¥å£"""
    
    # âœ¨ P2: é£é™©è¯„åˆ†æ¨¡å¼ (ç›´æ¥è¿”å› JSON)
    if request.mode == "risk_score":
        try:
            print("ğŸ“Š [Risk Scan] å¼€å§‹é£é™©è¯„ä¼°...")
            prompt = f"""
            ä½ æ˜¯ä¸€åèµ„æ·±æ³•å¾‹é£æ§ä¸“å®¶ã€‚è¯·é˜…è¯»ä»¥ä¸‹æ–‡ä¹¦ï¼Œä»å››ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆ0-100ï¼‰ã€‚
            ã€å¾…å®¡æ–‡ä¹¦ã€‘{request.current_doc[:3000]}
            ã€è¾“å‡ºè¦æ±‚ã€‘ä»…è¾“å‡ºæ ‡å‡† JSONï¼Œä¸è¦åŒ…å« Markdown æ ¼å¼æˆ–å…¶ä»–æ–‡å­—ï¼š
            {{
                "total_score": 85,
                "summary": "ä¸€å¥è¯ç®€è¯„ï¼ˆä¾‹å¦‚ï¼šæ•´ä½“åˆè§„ï¼Œä½†è¿çº¦è´£ä»»å¯¹ç”²æ–¹ä¸åˆ©ï¼‰",
                "dimensions": [
                    {{ "subject": "åˆè§„æ€§", "A": 90, "fullMark": 100 }},
                    {{ "subject": "æƒç›Šä¿æŠ¤", "A": 75, "fullMark": 100 }},
                    {{ "subject": "å®Œæ•´æ€§", "A": 85, "fullMark": 100 }},
                    {{ "subject": "æ–‡æœ¬è§„èŒƒ", "A": 95, "fullMark": 100 }}
                ]
            }}
            """
            completion = client.chat.completions.create(
                model="Qwen/Qwen2.5-32B-Instruct",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1, response_format={"type": "json_object"}
            )
            result = json.loads(completion.choices[0].message.content)
            return JSONResponse(result)
        except Exception as e:
            print(f"Risk scan failed: {e}")
            return JSONResponse({"error": "Analysis failed"}, status=500)

    # --- å¸¸è§„æµå¼æ¨¡å¼ ---
    last_user_msg = request.selection if request.mode == "selection_polish" else request.messages[-1].content
    user_id = request.user_id
    
    # 1. RAG æ£€ç´¢ (æ–‡æ¡£å¯¹è¯æ¨¡å¼ä¸‹ä¸æ£€ç´¢å¤–éƒ¨)
    rag_context = ""
    if request.mode != "selection_polish" and request.mode != "chat_doc":
        rag_context = get_relevant_laws_formatted(last_user_msg)

    # 2. è®°å¿†æ£€ç´¢
    memory_context = ""
    if user_id:
        memory_context = MemoryManager.retrieve_memories(user_id, last_user_msg)

    memory_section = f"ã€âš ï¸ ç”¨æˆ·åå¥½è®°å¿†ã€‘\n{memory_context}\n" if memory_context else ""
    rag_section = f"ã€ğŸ“š æƒå¨å‚è€ƒèµ„æ–™ã€‘\n{rag_context}\n" if rag_context else "ï¼ˆæ— ç‰¹å®šæ¡ˆä¾‹ï¼Œä¾é€šè¯†æ’°å†™ï¼‰"

    base_role = "ä½ æ˜¯ç”± LawLens å¼€å‘çš„ä¸­å›½é¡¶å°–æ³•å¾‹ AI åŠ©æ‰‹ã€‚"
    html_hint = "ä½¿ç”¨ HTML æ ‡ç­¾ (<h3>, <b>, <ul>, <blockquote>)ã€‚"

    system_instruction = ""

    if request.mode == "draft":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘èµ·è‰æ³•å¾‹æ–‡ä¹¦ã€‚
        {memory_section}
        {rag_section}
        {html_hint}
        ã€ç»“æ„ã€‘
        1. **æ€ç»´é“¾** (<blockquote>): åˆ†ææ¡ˆæƒ…ã€æ³•æ¡åŒ¹é…ã€è®°å¿†åº”ç”¨ã€‚
        2. **æ­£æ–‡**ï¼šå®Œæ•´æ–‡ä¹¦ã€‚
        """
    elif request.mode == "polish":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘å®¡æŸ¥æ¶¦è‰²ã€‚
        {memory_section}
        ã€æ–‡æ¡£ã€‘'''{request.current_doc}'''
        {rag_section}
        {html_hint}
        ã€ç»“æ„ã€‘
        1. **å®¡æŸ¥æ„è§** (<blockquote>): é£é™©ç‚¹ã€ä¿®æ”¹ä¾æ®ã€‚
        2. **ä¿®è®¢å…¨æ–‡**ï¼šç”¨ <b>åŠ ç²—</b> æ ‡æ³¨ä¿®æ”¹ã€‚
        """
    elif request.mode == "chat_doc": # âœ¨ P4: ä¸æ–‡æ¡£å¯¹è¯
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘æ ¹æ®å½“å‰æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚
        ã€æ–‡æ¡£å†…å®¹ã€‘'''{request.current_doc[:10000]}'''
        ã€ç”¨æˆ·é—®é¢˜ã€‘"{last_user_msg}"
        ã€è¦æ±‚ã€‘ç­”æ¡ˆå¿…é¡»åŸºäºæ–‡æ¡£å†…å®¹ï¼Œå¦‚æœæ–‡æ¡£æ²¡æåˆ°åˆ™è¯´ä¸çŸ¥é“ã€‚å¼•ç”¨åŸæ–‡æ—¶åŠ ç²—ã€‚
        """
    else: # selection_polish
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘å¾®è§‚æ¶¦è‰²ã€‚
        {memory_section}
        ã€åŸæ–‡ã€‘"{request.selection}"
        ã€æŒ‡ä»¤ã€‘"{last_user_msg}"
        ã€è¦æ±‚ã€‘ä»…è¾“å‡ºä¿®æ”¹åçš„æ–‡æœ¬ã€‚
        """

    messages = [{"role": "system", "content": system_instruction}]
    if request.mode != "selection_polish":
        history = [m.dict() for m in request.messages if m.role != "system"]
        messages.extend(history)
    else:
        messages.append({"role": "user", "content": last_user_msg})

    async def generate_stream():
        try:
            if request.mode == "draft":
                yield "<blockquote>ğŸ§  æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“... å›å¿†ç”¨æˆ·åå¥½...</blockquote>"
                time.sleep(0.5) # æ¨¡æ‹Ÿæ€è€ƒ

            stream = client.chat.completions.create(
                model="Qwen/Qwen2.5-32B-Instruct", 
                messages=messages,
                stream=True, 
                temperature=0.4,
                max_tokens=4000 
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            error_msg = f"<p style='color:red'>[AI ç”Ÿæˆé”™è¯¯: {str(e)}]</p>"
            print(f"âŒ AI Error: {e}")
            yield error_msg

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)