import os
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse 
from pydantic import BaseModel
from supabase import create_client, Client
from openai import OpenAI
from typing import List, Optional
import json

# ===========================
# 1. é…ç½®ä¸åˆå§‹åŒ– (å®Œå…¨ä¿æŒåŸæ ·)
# ===========================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")

supabase: Optional[Client] = None
client: Optional[OpenAI] = None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
def startup_event():
    global supabase, client
    if not all([SUPABASE_URL, SUPABASE_KEY, SILICONFLOW_API_KEY]):
        print("âŒ é”™è¯¯ï¼šæ ¸å¿ƒç¯å¢ƒå˜é‡ç¼ºå¤±")
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="https://api.siliconflow.cn/v1"
        )
        print("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (SiliconFlow / Qwen)")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

# ===========================
# 2. æ•°æ®æ¨¡å‹ (Pydantic) (å®Œå…¨ä¿æŒåŸæ ·)
# ===========================
class ChatMessage(BaseModel):
    role: str
    content: str

class AnalyzeRequest(BaseModel):
    messages: List[ChatMessage]
    current_doc: str = ""
    selection: Optional[str] = "" 
    mode: str = "draft"           # draft(ç”Ÿæˆ) | polish(æ¶¦è‰²) | selection_polish(å±€éƒ¨)

class DocumentSave(BaseModel):
    title: str
    content: str
    user_id: Optional[str] = None

# ===========================
# 3. è¾…åŠ©æ¥å£ (å†å² & ä¿å­˜) - (å®Œå…¨ä¿æŒåŸæ ·)
# ===========================
@app.post("/api/save")
async def save_document(doc: DocumentSave):
    """ä¿å­˜æ–‡æ¡£åˆ° Supabase"""
    if not supabase: return {"status": "error", "msg": "DBæœªè¿æ¥"}
    try:
        data = {"title": doc.title, "content": doc.content, "user_id": doc.user_id}
        supabase.table("documents").insert(data).execute()
        return {"status": "success"}
    except Exception as e:
        print(f"Save error: {e}")
        return {"status": "error", "msg": str(e)}

@app.get("/api/history")
async def get_history(user_id: Optional[str] = None):
    """è·å–å†å²è®°å½•"""
    if not supabase: return []
    try:
        query = supabase.table("documents").select("*").order("created_at", desc=True).limit(20)
        if user_id: query = query.eq("user_id", user_id)
        else: query = query.is_("user_id", "null")
        res = query.execute()
        return res.data
    except Exception as e:
        print(f"History error: {e}")
        return []

# ===========================
# 4. æ ¸å¿ƒ AI ä¸šåŠ¡é€»è¾‘ (ä¿®æ”¹éƒ¨åˆ†ï¼šPrompt é€»è¾‘)
# ===========================

def get_relevant_docs(query: str):
    """
    RAG æ£€ç´¢é€»è¾‘
    ä½¿ç”¨ SiliconFlow çš„ Embedding æ¨¡å‹å°†æŸ¥è¯¢å‘é‡åŒ–ï¼Œå» Supabase æœç´¢ç›¸ä¼¼æ¡ˆä¾‹/æ³•æ¡
    """
    if not client or not supabase: return []
    try:
        # ä½¿ç”¨ SiliconFlow æ”¯æŒçš„ embedding æ¨¡å‹ (ç¡®ä¿å’Œä½ æ•°æ®åº“å­˜çš„ä¸€è‡´)
        # æ³¨æ„ï¼šBAAI/bge-m3 ç”Ÿæˆçš„ç»´åº¦é€šå¸¸æ˜¯ 1024ï¼Œè¯·ç¡®ä¿ Supabase é‡Œçš„ embedding å­—æ®µç»´åº¦åŒ¹é…
        response = client.embeddings.create(model="BAAI/bge-m3", input=query)
        query_vector = response.data[0].embedding
        
        # è°ƒç”¨ Supabase RPC å‡½æ•°
        rpc_response = supabase.rpc("match_documents", {
            "query_embedding": query_vector,
            "match_threshold": 0.4, # ç¨å¾®æé«˜é˜ˆå€¼ï¼Œç¡®ä¿å‚è€ƒè´¨é‡
            "match_count": 3 
        }).execute()
        return rpc_response.data
    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        return []

@app.post("/api/analyze")
async def analyze(request: AnalyzeRequest):
    """æ ¸å¿ƒ AI åˆ†ææ¥å£ (æµå¼å“åº”)"""
    # ç¡®å®šç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾
    last_user_msg = request.selection or request.messages[-1].content
    print(f"ğŸ” è¯·æ±‚æ¨¡å¼: {request.mode} | æ„å›¾: {last_user_msg[:20]}...")
    
    # --- 1. RAG æ£€ç´¢ (å…ˆçœ‹æ¡ˆä¾‹) ---
    context_text = ""
    # å±€éƒ¨æ¶¦è‰²é€šå¸¸ä¸éœ€è¦æŸ¥å¤§æ¡ˆä¾‹ï¼Œé™¤ééå¸¸æ¨¡ç³Šï¼›è‰æ‹Ÿå’Œå…¨æ–‡æ¶¦è‰²å¿…é¡»æŸ¥
    if request.mode != "selection_polish":
        relevant_docs = get_relevant_docs(last_user_msg)
        if relevant_docs:
            doc_snippets = []
            for i, d in enumerate(relevant_docs):
                # å‡è®¾æ•°æ®åº“å­—æ®µæœ‰ title/law_name å’Œ content
                # è¿™é‡Œåšä¸ªå…¼å®¹ï¼Œå¦‚æœæ²¡æœ‰ law_name å°±ç”¨ id
                source = d.get('law_name') or d.get('title') or f"æ¡ˆä¾‹ #{d.get('id')}"
                doc_snippets.append(f"ã€å‚è€ƒèµ„æ–™ {i+1} ({source})ã€‘:\n{d['content']}")
            
            context_text = "\n\n".join(doc_snippets)
            print(f"âœ… å·²æ³¨å…¥ {len(relevant_docs)} æ¡å‚è€ƒèµ„æ–™")
        else:
            print("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³èµ„æ–™ï¼Œä½¿ç”¨é€šç”¨é€»è¾‘")

    # --- 2. Prompt æ„å»º (æ ¸å¿ƒä¿®æ”¹ï¼šæ€ç»´é“¾) ---
    base_role = "ä½ æ˜¯ä¸€åä¸­å›½çº¢åœˆå¾‹æ‰€é«˜çº§åˆä¼™äººï¼Œä¸“ç²¾äºæ°‘å•†äº‹æ³•å¾‹æ–‡ä¹¦å†™ä½œã€‚"
    html_hint = "è¯·ç›´æ¥è¾“å‡º HTML æ ¼å¼ï¼ˆ<p>, <b>, <br>ï¼‰ï¼Œä¸è¦ä½¿ç”¨ Markdown ä»£ç å—ã€‚"

    # è¿™é‡Œçš„ Prompt ä¸¥æ ¼éµå¾ªï¼šåˆ†æå‚è€ƒèµ„æ–™ -> æ¨¡ä»¿é€»è¾‘ -> æ‰§è¡Œå†™ä½œ
    if request.mode == "selection_polish":
        # Case A: å±€éƒ¨æ¶¦è‰²
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘ç”¨æˆ·é€‰ä¸­äº†æ–‡æ¡£ä¸­çš„ä¸€æ®µè¯ï¼Œè¯·å¯¹å…¶è¿›è¡Œã€å¾®è§‚æ¶¦è‰²ã€‘ã€‚
        
        ã€é€‰ä¸­åŸæ–‡ã€‘
        "{request.selection}"
        
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘
        {request.messages[-1].content}
        
        ã€è¦æ±‚ã€‘
        1. **ä»…è¾“å‡ºä¿®æ”¹åçš„é‚£ä¸€æ®µè¯**ï¼Œä¸¥ç¦è¾“å‡ºä»»ä½•è§£é‡Šã€é¦–å°¾å¯’æš„ã€‚
        2. ä¿æŒ HTML æ ¼å¼ã€‚
        3. è¯­æ°”ä¸¥è°¨ã€æœ‰åŠ›ï¼Œæ¶ˆé™¤å£è¯­åŒ–è¡¨è¾¾ï¼Œä½¿ç”¨æ³•è¨€æ³•è¯­ã€‚
        """
    
    elif request.mode == "polish":
        # Case B: å…¨æ–‡æ¶¦è‰²
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘è¯·ä¾æ®ä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™åº“ã€‘ï¼Œå¯¹ç”¨æˆ·æä¾›çš„æ•´ç¯‡æ–‡ä¹¦è¿›è¡Œæ·±åº¦æ¶¦è‰²ã€‚
        
        ã€å‚è€ƒèµ„æ–™åº“ï¼ˆè¿™æ˜¯ä½ çš„çŸ¥è¯†æºï¼‰ã€‘
        {context_text if context_text else "ï¼ˆæš‚æ— ç‰¹å®šå‚è€ƒæ¡ˆä¾‹ï¼Œè¯·ä¾æ®ã€Šæ°‘æ³•å…¸ã€‹åŠå®åŠ¡ç»éªŒï¼‰"}
        
        ã€å¾…æ¶¦è‰²æ–‡æ¡£ã€‘
        '''{request.current_doc}'''
        
        ã€æ‰§è¡Œæ­¥éª¤ã€‘
        1. **å¯¹æ¯”åˆ†æ**ï¼šå¯¹æ¯”å¾…æ¶¦è‰²æ–‡æ¡£ä¸å‚è€ƒèµ„æ–™ï¼Œæ£€æŸ¥ç”¨è¯æ˜¯å¦å¤Ÿä¸“ä¸šï¼Œé€»è¾‘æ˜¯å¦åƒå‚è€ƒæ¡ˆä¾‹é‚£æ ·ä¸¥å¯†ã€‚
        2. **æ‰§è¡Œä¿®æ”¹**ï¼šä¿ç•™åŸæ„ï¼Œä½†å°†æªè¾æå‡è‡³ä¸“ä¸šå¾‹å¸ˆæ°´å‡†ã€‚
        3. **æ ¼å¼è¾“å‡º**ï¼š{html_hint}
        """
        
    else: 
        # Case C: ä»é›¶ç”Ÿæˆ (Draft) - è¿™æ˜¯ä½ æœ€çœ‹é‡çš„é€»è¾‘
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œå‚è€ƒç±»ä¼¼æ¡ˆä¾‹çš„å†™æ³•ï¼Œä»é›¶èµ·è‰æ³•å¾‹æ–‡ä¹¦ã€‚
        
        ã€å‚è€ƒèµ„æ–™åº“ï¼ˆçœŸå®æ¡ˆä¾‹ä¸æ³•æ¡ï¼‰ã€‘
        {context_text if context_text else "ï¼ˆæœ¬æ¬¡æ£€ç´¢æœªæ‰¾åˆ°é«˜åº¦ç›¸ä¼¼æ¡ˆä¾‹ï¼Œè¯·ä¾æ®é€šç”¨æ³•å¾‹å®åŠ¡æ’°å†™ï¼‰"}
        
        ã€å·¥ä½œæµã€‘
        1. **æ£€ç´¢åˆ†æ**ï¼šé˜…è¯»ä¸Šè¿°ã€å‚è€ƒèµ„æ–™åº“ã€‘ï¼Œå­¦ä¹ å…¶è¯‰è®¼è¯·æ±‚çš„è¡¨è¿°æ–¹å¼ã€äº‹å®é™ˆè¿°çš„é€»è¾‘ç»“æ„ä»¥åŠå¼•ç”¨çš„æ³•å¾‹æ¡æ¬¾ã€‚
        2. **é€»è¾‘è¿ç§»**ï¼šå°†å‚è€ƒæ¡ˆä¾‹ä¸­çš„ä¼˜ç§€é€»è¾‘è¿ç§»åˆ°æœ¬æ¡ˆä¸­ã€‚
        3. **æ’°å†™æ–‡ä¹¦**ï¼š
           - ç»“æ„å¿…é¡»å®Œæ•´ï¼ˆé¦–éƒ¨ã€äº‹å®ä¸ç†ç”±ã€è¯‰è®¼è¯·æ±‚/æ¡æ¬¾ã€å°¾éƒ¨ï¼‰ã€‚
           - å¿…é¡»å¼•ç”¨é€‚ç”¨çš„æ³•å¾‹æ¡æ¬¾ã€‚
           - ä¸¥ç¦å£è¯­åŒ–ï¼Œå¿…é¡»ä½¿ç”¨æ³•è¨€æ³•è¯­ã€‚
        
        ã€è¾“å‡ºè¦æ±‚ã€‘
        {html_hint}
        """

    # --- 3. æ¶ˆæ¯å†å²å¤„ç† ---
    llm_messages = [{"role": "system", "content": system_instruction}]
    
    if request.mode == "selection_polish":
        pass # å±€éƒ¨æ¨¡å¼ Prompt å·²ç»åŒ…å«äº†æ‰€æœ‰ä¿¡æ¯
    else:
        # å…¶ä»–æ¨¡å¼å¸¦ä¸Šå†å²è®°å½•ï¼Œæ”¯æŒè¿½é—®
        llm_messages.extend([m.dict() for m in request.messages if m.role != 'system'])

    # --- 4. æµå¼ç”Ÿæˆå™¨ ---
    async def generate_stream():
        try:
            stream = client.chat.completions.create(
                model="Qwen/Qwen2.5-32B-Instruct", # ä½ çš„ SiliconFlow æ¨¡å‹
                messages=llm_messages,
                stream=True, 
                temperature=0.3, # æ³•å¾‹æ–‡ä¹¦å»ºè®®è°ƒä½æ¸©åº¦ï¼Œæ›´ä¸¥è°¨
                max_tokens=4000  # æ–‡ä¹¦å¯èƒ½è¾ƒé•¿
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            error_msg = f"<p style='color:red'>[AI ç”Ÿæˆé”™è¯¯: {str(e)}]</p>"
            print(f"âŒ AI Error: {e}")
            yield error_msg

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)