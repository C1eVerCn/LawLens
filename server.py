import os
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse 
from pydantic import BaseModel
from supabase import create_client, Client
from openai import OpenAI
from typing import List, Optional
import json

# ===========================
# 1. é…ç½®ä¸åˆå§‹åŒ–
# ===========================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")

supabase: Optional[Client] = None
client: Optional[OpenAI] = None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
def startup_event():
    global supabase, client
    if not all([SUPABASE_URL, SUPABASE_KEY, SILICONFLOW_API_KEY]):
        print("âŒ é”™è¯¯ï¼šæ ¸å¿ƒç¯å¢ƒå˜é‡ç¼ºå¤±")
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="https://api.siliconflow.cn/v1"
        )
        print("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (SiliconFlow / Qwen)")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

# ===========================
# 2. æ•°æ®æ¨¡å‹ (Pydantic)
# ===========================
class ChatMessage(BaseModel):
    role: str
    content: str

class AnalyzeRequest(BaseModel):
    messages: List[ChatMessage]
    current_doc: str = ""
    selection: Optional[str] = "" # ğŸ‘ˆ æ–°å¢ï¼šæ”¯æŒå±€éƒ¨é€‰ä¸­çš„æ–‡æœ¬
    mode: str = "draft"           # draft(ç”Ÿæˆ) | polish(æ¶¦è‰²) | selection_polish(å±€éƒ¨)

class DocumentSave(BaseModel):
    title: str
    content: str
    user_id: Optional[str] = None

# ===========================
# 3. è¾…åŠ©æ¥å£ (å†å² & ä¿å­˜) - å®Œæ•´ä¿ç•™
# ===========================
@app.post("/api/save")
async def save_document(doc: DocumentSave):
    """ä¿å­˜æ–‡æ¡£åˆ° Supabase"""
    if not supabase: return {"status": "error", "msg": "DBæœªè¿æ¥"}
    try:
        data = {"title": doc.title, "content": doc.content, "user_id": doc.user_id}
        supabase.table("documents").insert(data).execute()
        return {"status": "success"}
    except Exception as e:
        print(f"Save error: {e}")
        return {"status": "error", "msg": str(e)}

@app.get("/api/history")
async def get_history(user_id: Optional[str] = None):
    """è·å–å†å²è®°å½•"""
    if not supabase: return []
    try:
        query = supabase.table("documents").select("*").order("created_at", desc=True).limit(20)
        if user_id: query = query.eq("user_id", user_id)
        else: query = query.is_("user_id", "null")
        res = query.execute()
        return res.data
    except Exception as e:
        print(f"History error: {e}")
        return []

# ===========================
# 4. æ ¸å¿ƒ AI ä¸šåŠ¡é€»è¾‘
# ===========================

def get_relevant_laws(query: str):
    """RAG æ£€ç´¢é€»è¾‘"""
    if not client or not supabase: return []
    try:
        response = client.embeddings.create(model="BAAI/bge-m3", input=query)
        query_vector = response.data[0].embedding
        
        # é’ˆå¯¹å±€éƒ¨æ¶¦è‰²å‡å°‘æ£€ç´¢é‡ï¼Œæé«˜é€Ÿåº¦
        rpc_response = supabase.rpc("match_documents", {
            "query_embedding": query_vector,
            "match_threshold": 0.35,
            "match_count": 3 
        }).execute()
        return rpc_response.data
    except Exception as e:
        print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
        return []

@app.post("/api/analyze")
async def analyze(request: AnalyzeRequest):
    """æ ¸å¿ƒ AI åˆ†ææ¥å£ (æµå¼å“åº”)"""
    last_user_msg = request.messages[-1].content
    print(f"ğŸ” è¯·æ±‚æ¨¡å¼: {request.mode} | é•¿åº¦: {len(last_user_msg)}")
    
    # --- ä¸Šä¸‹æ–‡æ„å»º (RAG) ---
    context_text = ""
    # åªæœ‰åœ¨éå±€éƒ¨æ¨¡å¼ä¸‹æ‰è¿›è¡Œé‡åº¦æ£€ç´¢ï¼Œé¿å…å±€éƒ¨æ¶¦è‰²æ—¶è¢«æ— å…³æ¡ˆä¾‹å¹²æ‰°
    if request.mode != "selection_polish":
        relevant_docs = get_relevant_laws(last_user_msg)
        if relevant_docs:
            context_text = "ã€æƒå¨æ³•å¾‹ä¾æ®åº“ã€‘\n" + "\n".join(
                [f"ã€Š{d['law_name']}ã€‹: {d['content'][:300]}..." for d in relevant_docs]
            )

    # --- Prompt æ„å»º ---
    base_role = "ä½ æ˜¯ä¸€åä¸­å›½çº¢åœˆå¾‹æ‰€é«˜çº§åˆä¼™äººï¼Œä¸“ç²¾äºæ³•å¾‹æ–‡ä¹¦å†™ä½œã€‚"
    html_hint = "è¯·ç›´æ¥è¾“å‡º HTML æ ¼å¼ï¼ˆ<p>, <b>, <br>ï¼‰ï¼Œä¸è¦ä½¿ç”¨ Markdown ä»£ç å—ã€‚"

    if request.mode == "selection_polish":
        # Case A: å±€éƒ¨æ¶¦è‰²
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘ç”¨æˆ·é€‰ä¸­äº†æ–‡æ¡£ä¸­çš„ä¸€æ®µè¯ï¼Œè¯·å¯¹å…¶è¿›è¡Œã€å¾®è§‚æ¶¦è‰²ã€‘ã€‚
        
        ã€é€‰ä¸­åŸæ–‡ã€‘
        "{request.selection}"
        
        ã€ç”¨æˆ·æŒ‡ä»¤ã€‘
        {last_user_msg} (è‹¥æ— å…·ä½“æŒ‡ä»¤ï¼Œé»˜è®¤è¿›è¡Œä¸“ä¸šåŒ–ã€æ³•è¨€æ³•è¯­è§„èŒƒåŒ–ä¿®æ”¹)
        
        ã€è¦æ±‚ã€‘
        1. **ä»…è¾“å‡ºä¿®æ”¹åçš„é‚£ä¸€æ®µè¯**ï¼Œä¸¥ç¦è¾“å‡ºä»»ä½•è§£é‡Šã€é¦–å°¾å¯’æš„ã€‚
        2. ä¿æŒ HTML æ ¼å¼ã€‚
        3. è¯­æ°”ä¸¥è°¨ã€æœ‰åŠ›ï¼Œä¸æ”¹å˜åŸæ„ã€‚
        """
    
    elif request.mode == "polish":
        # Case B: å…¨æ–‡æ¶¦è‰²
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘å¯¹æ•´ç¯‡æ–‡ä¹¦è¿›è¡Œä¸“ä¸šæ¶¦è‰²ã€‚
        ã€å½“å‰æ–‡æ¡£ã€‘'''{request.current_doc}'''
        ã€è¦æ±‚ã€‘æœ¯è¯­ä¸“ä¸šåŒ–ï¼Œé€»è¾‘ä¸¥å¯†ï¼ŒHTMLæ’ç‰ˆã€‚é‡ç‚¹å†…å®¹åŠ ç²—ã€‚
        {html_hint}
        {context_text}
        """
        
    else: 
        # Case C: ä»é›¶ç”Ÿæˆ (Draft)
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘ä»é›¶èµ·è‰æ³•å¾‹æ–‡ä¹¦ã€‚
        ã€è¦æ±‚ã€‘ç»“æ„å®Œå¤‡ï¼Œå¼•ç”¨è§„èŒƒï¼ŒHTMLæ’ç‰ˆã€‚
        {html_hint}
        {context_text}
        """

    # --- æ¶ˆæ¯å†å²å¤„ç† ---
    llm_messages = [{"role": "system", "content": system_instruction}]
    
    if request.mode == "selection_polish":
        # å±€éƒ¨æ¨¡å¼ä¸‹ï¼Œåªä¿ç•™å½“å‰æŒ‡ä»¤ï¼Œé¿å…è¢«ä¹‹å‰çš„é•¿å¯¹è¯å¹²æ‰°
        msg_content = last_user_msg if last_user_msg else "è¯·ä¸“ä¸šåŒ–æ¶¦è‰²è¿™æ®µæ–‡å­—"
        llm_messages.append({"role": "user", "content": msg_content})
    else:
        # å…¶ä»–æ¨¡å¼å¸¦ä¸Šå†å²è®°å½•ï¼Œæ”¯æŒè¿½é—®
        llm_messages.extend([m.dict() for m in request.messages if m.role != 'system'])

    # --- æµå¼ç”Ÿæˆå™¨ ---
    async def generate_stream():
        try:
            stream = client.chat.completions.create(
                model="Qwen/Qwen2.5-32B-Instruct", 
                messages=llm_messages,
                stream=True, 
                temperature=0.7,
                max_tokens=2000 
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"<p style='color:red'>[Error: {str(e)}]</p>"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)