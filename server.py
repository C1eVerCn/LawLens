import os
import uvicorn
import time
import json
import mammoth
import io
import re
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from supabase import create_client, Client
from openai import OpenAI
from typing import List, Optional

# ===========================
# 1. é…ç½®ä¸åˆå§‹åŒ–
# ===========================
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")

# âœ¨ ä½¿ç”¨ Qwen 2.5 72B (å½“å‰å¼€æºæœ€å¼ºï¼Œç›¸å½“äº Max)
MODEL_NAME = "Qwen/Qwen2.5-72B-Instruct"

supabase: Optional[Client] = None
client: Optional[OpenAI] = None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
def startup_event():
    global supabase, client
    if not all([SUPABASE_URL, SUPABASE_KEY, SILICONFLOW_API_KEY]):
        print("âŒ é”™è¯¯ï¼šæ ¸å¿ƒç¯å¢ƒå˜é‡ç¼ºå¤±")
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url="[https://api.siliconflow.cn/v1](https://api.siliconflow.cn/v1)"
        )
        print(f"âœ… LawLens æ™ºèƒ½å¼•æ“å·²å¯åŠ¨ (æ¨¡å‹: {MODEL_NAME})")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")

# ===========================
# 2. å·¥å…·å‡½æ•°ï¼šJSON æ¸…æ´— (æ ¸å¿ƒä¿®å¤)
# ===========================
def clean_json_output(content: str):
    """
    æ¸…æ´—å¤§æ¨¡å‹è¿”å›çš„ JSON å­—ç¬¦ä¸²ï¼Œå»é™¤ Markdown æ ‡è®°
    """
    try:
        # 1. å°è¯•å»é™¤ markdown ä»£ç å—
        if "```" in content:
            # åŒ¹é… ```json ... ``` æˆ– ``` ... ``` ä¸­é—´çš„å†…å®¹
            pattern = r"```(?:json)?\s*(.*?)\s*```"
            match = re.search(pattern, content, re.DOTALL)
            if match:
                content = match.group(1)
        
        # 2. å»é™¤é¦–å°¾ç©ºç™½
        content = content.strip()
        
        # 3. å°è¯•è§£æ
        return json.loads(content)
    except Exception as e:
        print(f"âŒ JSON è§£æå¤±è´¥: {e}\nåŸå§‹å†…å®¹: {content}")
        # å…œåº•è¿”å›ä¸€ä¸ªç©ºç»“æ„ï¼Œé˜²æ­¢å‰ç«¯å´©æºƒ
        return {
            "total_score": 0,
            "summary": "AI è§£ææ•°æ®æ ¼å¼å¼‚å¸¸ï¼Œè¯·é‡è¯•ã€‚",
            "dimensions": []
        }

# ===========================
# 3. æ•°æ®æ¨¡å‹
# ===========================
class ChatMessage(BaseModel):
    role: str
    content: str

class AnalyzeRequest(BaseModel):
    messages: List[ChatMessage]
    current_doc: str = ""
    selection: Optional[str] = "" 
    mode: str = "draft" 
    user_id: Optional[str] = None 

class DocumentSave(BaseModel):
    title: str
    content: str
    user_id: Optional[str] = None

class MemoryCreate(BaseModel):
    user_id: str
    content: str
    type: str = "preference"

# ===========================
# 4. ğŸ§  Memory Manager
# ===========================
class MemoryManager:
    @staticmethod
    def add_memory(user_id: str, content: str, m_type: str = "preference"):
        if not client or not supabase: return False
        try:
            resp = client.embeddings.create(model="BAAI/bge-m3", input=content)
            vec = resp.data[0].embedding
            supabase.table("agent_memories").insert({
                "user_id": user_id, "content": content, "memory_type": m_type, "embedding": vec
            }).execute()
            return True
        except Exception: return False

    @staticmethod
    def retrieve_memories(user_id: str, query: str) -> str:
        if not client or not supabase or not user_id: return ""
        try:
            resp = client.embeddings.create(model="BAAI/bge-m3", input=query)
            vec = resp.data[0].embedding
            rpc_resp = supabase.rpc("match_memories", {
                "query_embedding": vec, "match_threshold": 0.5, "match_count": 3, "p_user_id": user_id
            }).execute()
            if not rpc_resp.data: return ""
            return "\n".join([f"- {m['content']}" for m in rpc_resp.data])
        except Exception: return ""

# ===========================
# 5. è¾…åŠ©æ¥å£
# ===========================
@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        content = await file.read()
        result = mammoth.convert_to_html(io.BytesIO(content))
        return {"status": "success", "content": result.value}
    except Exception as e:
        return {"status": "error", "msg": "æ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·ç¡®ä¿æ˜¯ .docx æ–‡ä»¶"}

@app.post("/api/memory")
async def create_memory(mem: MemoryCreate):
    success = MemoryManager.add_memory(mem.user_id, mem.content, mem.type)
    return {"status": "success" if success else "error"}

@app.post("/api/save")
async def save_document(doc: DocumentSave):
    if not supabase: return {"status": "error", "msg": "DBæœªè¿æ¥"}
    try:
        raw_text = doc.content.replace('<', '').replace('>', '')[:20]
        title = doc.title if doc.title and doc.title != "æœªå‘½åæ³•å¾‹æ–‡ä¹¦" else f"{raw_text}..."
        supabase.table("documents").insert({"title": title, "content": doc.content, "user_id": doc.user_id}).execute()
        return {"status": "success"}
    except Exception as e:
        return {"status": "error", "msg": str(e)}

@app.get("/api/history")
async def get_history(user_id: Optional[str] = None):
    if not supabase: return []
    try:
        query = supabase.table("documents").select("*").order("created_at", desc=True).limit(20)
        if user_id: query = query.eq("user_id", user_id)
        else: query = query.is_("user_id", "null")
        return query.execute().data
    except Exception: return []

# ===========================
# 6. æ ¸å¿ƒ AI é€»è¾‘
# ===========================

def get_rag_context(query: str):
    if not client or not supabase: return ""
    try:
        resp = client.embeddings.create(model="BAAI/bge-m3", input=query)
        vec = resp.data[0].embedding
        rpc_resp = supabase.rpc("match_documents", {
            "query_embedding": vec, "match_threshold": 0.45, "match_count": 3 
        }).execute()
        
        if not rpc_resp.data: return ""
        formatted = ""
        for i, doc in enumerate(rpc_resp.data):
            snippet = doc['content'][:500].replace('\n', ' ')
            formatted += f"ã€å‚è€ƒèµ„æ–™ {i+1}ã€‘\n{snippet}...\n"
        return formatted
    except Exception: return ""

@app.post("/api/analyze")
async def analyze(request: AnalyzeRequest):
    """æ ¸å¿ƒ AI æ¥å£"""
    
    # --- P2: é£é™©è¯„åˆ† (JSON) ---
    if request.mode == "risk_score":
        try:
            print("ğŸ“Š [Risk Scan] æ­£åœ¨è°ƒç”¨ Qwen 72B è¿›è¡Œä½“æ£€...")
            
            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„ Prompt å¼•å¯¼æ¨¡å‹è¾“å‡ºçº¯ JSON
            prompt = f"""
            ä½ æ˜¯ä¸€åèµ„æ·±çš„æ³•å¾‹åˆè§„ä¸“å®¶ã€‚è¯·ä»”ç»†å®¡æŸ¥ä»¥ä¸‹æ³•å¾‹æ–‡ä¹¦ç‰‡æ®µï¼Œå¹¶ä»å››ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰ã€‚
            
            ã€å¾…å®¡æ–‡ä¹¦å†…å®¹ã€‘
            {request.current_doc[:4000]} 
            
            ã€ä»»åŠ¡è¦æ±‚ã€‘
            è¯·ç›´æ¥è¿”å›ä¸€ä¸ªæ ‡å‡†çš„ JSON å¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½• Markdown æ ¼å¼ï¼ˆå¦‚ ```jsonï¼‰ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šæ–‡å­—ã€‚
            
            JSON æ•°æ®ç»“æ„å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
            {{
                "total_score": 85,
                "summary": "è¿™é‡Œå†™ä¸€å¥è¯çš„ä¸­æ–‡ç®€è¯„ï¼ŒæŒ‡å‡ºä¸»è¦é£é™©æˆ–ä¼˜ç‚¹ã€‚",
                "dimensions": [
                    {{ "subject": "åˆè§„æ€§", "A": 90, "fullMark": 100 }},
                    {{ "subject": "æƒç›Šä¿æŠ¤", "A": 75, "fullMark": 100 }},
                    {{ "subject": "å®Œæ•´æ€§", "A": 85, "fullMark": 100 }},
                    {{ "subject": "æ–‡æœ¬è§„èŒƒ", "A": 95, "fullMark": 100 }}
                ]
            }}
            """
            
            completion = client.chat.completions.create(
                model=MODEL_NAME, # Qwen/Qwen2.5-72B-Instruct
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåªè¾“å‡º JSON æ ¼å¼çš„ API æ¥å£ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1, 
                # ç§»é™¤ response_format å‚æ•°ï¼Œé˜²æ­¢éƒ¨åˆ†æ¨¡å‹ç½‘å…³æŠ¥é”™ï¼Œæ”¹ç”¨æ‰‹åŠ¨æ¸…æ´—
            )
            
            raw_content = completion.choices[0].message.content
            print(f"ğŸ“¥ [Raw AI Output]: {raw_content[:100]}...") # æ‰“å°æ—¥å¿—æ–¹ä¾¿è°ƒè¯•
            
            # æ¸…æ´—å¹¶è§£æ JSON
            result = clean_json_output(raw_content)
            
            return JSONResponse(result)
            
        except Exception as e:
            print(f"âŒ Risk scan failed: {e}")
            return JSONResponse({"error": "ä½“æ£€æœåŠ¡æš‚æ—¶ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•"}, status=500)

    # --- å¸¸è§„æµå¼æ¨¡å¼ ---
    last_user_msg = request.selection if request.mode == "selection_polish" else request.messages[-1].content
    user_id = request.user_id
    
    rag_context = ""
    found_cases = False
    if request.mode != "selection_polish" and request.mode != "chat_doc":
        rag_context = get_rag_context(last_user_msg)
        if rag_context: found_cases = True

    memory_context = ""
    if user_id:
        memory_context = MemoryManager.retrieve_memories(user_id, last_user_msg)

    memory_section = f"ã€âš ï¸ ç”¨æˆ·åå¥½è®°å¿†ã€‘\nè¯·ä¸¥æ ¼éµå®ˆï¼š{memory_context}\n" if memory_context else ""
    rag_section = f"ã€ğŸ“š æ³•å¾‹æ•°æ®åº“ã€‘\n{rag_context}\n" if rag_context else "ï¼ˆä½¿ç”¨é€šç”¨æ³•å¾‹çŸ¥è¯†ï¼‰"

    base_role = "ä½ æ˜¯ç”± LawLens å¼€å‘çš„ä¸­å›½é¡¶å°–æ³•å¾‹ AI åŠ©æ‰‹ã€‚ä½ çš„å›ç­”å¿…é¡»ä¸“ä¸šã€ä¸¥è°¨ã€ç¬¦åˆä¸­å›½æ³•å¾‹è§„èŒƒã€‚"
    html_hint = "ä½¿ç”¨ HTML æ ‡ç­¾æ’ç‰ˆ (<h3>, <b>, <ul>, <blockquote>)ï¼Œç¦æ­¢ Markdownã€‚"

    system_instruction = ""

    if request.mode == "draft":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘èµ·è‰æ³•å¾‹æ–‡ä¹¦ã€‚
        {memory_section}
        {rag_section}
        {html_hint}
        ã€è¾“å‡ºç»“æ„ã€‘
        1. **åˆ†ææŠ¥å‘Š** (<blockquote>): æ ¸å¿ƒäº‰è®®ç‚¹ã€æ³•å¾‹ä¾æ®ã€èµ·è‰ç­–ç•¥ã€‚
        2. **æ­£å¼æ–‡ä¹¦**: å®Œæ•´çš„åˆåŒæˆ–å‡½ä»¶ã€‚
        """
    elif request.mode == "polish":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘å®¡æŸ¥å¹¶æ¶¦è‰²ã€‚
        {memory_section}
        ã€æ–‡æ¡£ã€‘'''{request.current_doc}'''
        {rag_section}
        {html_hint}
        ã€è¾“å‡ºç»“æ„ã€‘
        1. **å®¡æŸ¥æ„è§** (<blockquote>): é£é™©æç¤ºã€ä¿®æ”¹ä¾æ®ã€‚
        2. **ä¿®è®¢å…¨æ–‡**: ç”¨ <b>åŠ ç²—</b> æ ‡è®°ä¿®æ”¹å¤„ã€‚
        """
    elif request.mode == "chat_doc":
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘åŸºäºæ–‡æ¡£å›ç­”é—®é¢˜ã€‚
        ã€æ–‡æ¡£ã€‘'''{request.current_doc[:10000]}'''
        ã€é—®é¢˜ã€‘"{last_user_msg}"
        ã€è¦æ±‚ã€‘ç­”æ¡ˆå¿…é¡»åŸºäºæ–‡æ¡£ï¼Œå¼•ç”¨å¤„åŠ ç²—ã€‚
        """
    else: 
        system_instruction = f"""
        {base_role}
        ã€ä»»åŠ¡ã€‘å¾®è°ƒé€‰ä¸­æ–‡æœ¬ã€‚
        {memory_section}
        ã€åŸæ–‡ã€‘"{request.selection}"
        ã€æŒ‡ä»¤ã€‘"{last_user_msg}"
        ã€è¦æ±‚ã€‘ä»…è¾“å‡ºä¿®æ”¹åçš„æ–‡æœ¬ã€‚
        """

    messages = [{"role": "system", "content": system_instruction}]
    if request.mode != "selection_polish":
        history = [m.dict() for m in request.messages if m.role != "system"]
        messages.extend(history)
    else:
        messages.append({"role": "user", "content": last_user_msg})

    async def generate_stream():
        try:
            if request.mode != "selection_polish":
                status_rag = f"âœ… å·²åŒ¹é… {rag_context.count('ã€å‚è€ƒèµ„æ–™')} ä¸ªç›¸å…³æ¡ˆä¾‹" if found_cases else "âš ï¸ é€šç”¨æ³•å¾‹æ¨¡å¼"
                status_mem = "âœ… å‘½ä¸­åå¥½" if memory_context else "æ— ç‰¹å®šåå¥½"
                status_html = f"""
                <div style="background:#f8fafc; padding:12px; border-radius:8px; border:1px solid #e2e8f0; margin-bottom:16px; font-size:13px; color:#475569;">
                    <div style="display:flex; align-items:center; gap:8px; margin-bottom:8px;">
                        <span style="display:inline-block; width:8px; height:8px; background:#2563eb; border-radius:50%;"></span>
                        <b>AI æ³•å¾‹å¼•æ“è¿è¡Œä¸­...</b>
                    </div>
                    <ul style="margin:0; padding-left:20px; line-height: 1.6;">
                        <li>åˆ†ææ¡ˆæƒ…ï¼š{last_user_msg[:10]}...</li>
                        <li>æ•°æ®åº“ï¼š{status_rag}</li>
                        <li>è®°å¿†åº“ï¼š{status_mem}</li>
                    </ul>
                </div>
                """
                yield status_html
                time.sleep(0.5)

            stream = client.chat.completions.create(
                model=MODEL_NAME, 
                messages=messages,
                stream=True, 
                temperature=0.4,
                max_tokens=4000 
            )
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"<p style='color:red'>AI æœåŠ¡å“åº”é”™è¯¯: {str(e)}</p>"

    return StreamingResponse(generate_stream(), media_type="text/event-stream")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)